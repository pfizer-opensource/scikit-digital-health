{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "\n",
    "from PfyMU.gait.train_classifier.core import load_datasets\n",
    "from PfyMU.features import *\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_band_filter(x, fs):\n",
    "    sos = butter(1, [2 * 0.25 / fs, 2 * 5 / fs], btype='band', output='sos')\n",
    "    return sosfiltfilt(sos, np.linalg.norm(x, axis=1))\n",
    "\n",
    "steps = {\n",
    "    'walking': 0.4,\n",
    "    'walking-impaired': 0.2,\n",
    "    'sitting': 900,\n",
    "    'standing': 300,\n",
    "    'stairs-ascending': 0.3,\n",
    "    'stairs-descending': 0.3,\n",
    "    'cycling-50W': 0.3,\n",
    "    'cycling-100W': 0.3,\n",
    "    'default': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gait_sets_path = Path('/Users/adamol/Documents/Datasets/gait/processed')\n",
    "gait_sets_path = Path('/home/lukasadamowicz/Documents/Datasets/processed')\n",
    "\n",
    "datasets = [\n",
    "    gait_sets_path / 'bluesky2',\n",
    "    gait_sets_path / 'daliac',\n",
    "    gait_sets_path / 'ltmm',\n",
    "    gait_sets_path / 'usc-had'\n",
    "]\n",
    "\n",
    "kwargs = {'paths': datasets, 'goal_fs': 50.0, 'window_step': steps, 'window_length': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, subjects, activities = load_datasets(\n",
    "    acc_mag=False, \n",
    "    signal_function=mag_band_filter,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(398)\n",
    "rnd_subjects = [i for i in np.unique(subjects) if np.unique(activities[subjects==i]).size > 3]\n",
    "random.shuffle(rnd_subjects)\n",
    "\n",
    "training_masks, validation_masks, testing_masks = [], [], []\n",
    "\n",
    "for i in range(0, len(rnd_subjects), 4):\n",
    "    trm = np.ones(len(subjects), dtype='bool')\n",
    "    vm = np.zeros_like(trm, dtype='bool')\n",
    "    tem = np.zeros_like(trm, dtype='bool')\n",
    "    \n",
    "    for j in range(4):\n",
    "        trm &= subjects != rnd_subjects[i + j]\n",
    "        if j < 2:\n",
    "            vm |= subjects == rnd_subjects[i + j]\n",
    "        else:\n",
    "            tem |= subjects == rnd_subjects[i + j]\n",
    "    \n",
    "    training_masks.append(trm)\n",
    "    validation_masks.append(vm)\n",
    "    testing_masks.append(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominantfrequency_1.00_3.50\n",
      "meancrossrate\n",
      "range\n",
      "rms\n",
      "sampleentropy_2_0.50\n",
      "autocorrelation_15_True\n",
      "iqr\n",
      "rangecountpercentage_0.40_1.50\n",
      "complexityinvariantdistance_True\n",
      "permutationentropy_3_1_True\n",
      "spectralentropy_0.00_5.00\n",
      "spectralflatness_0.00_6.00\n",
      "mean\n",
      "jerkmetric\n",
      "dimensionlessjerk_True_acceleration\n",
      "signalentropy\n",
      "sparc_4_10.00_0.05\n",
      "linearslope\n"
     ]
    }
   ],
   "source": [
    "with open('lgb_features.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB = Bank(window_length=None, window_step=None)\n",
    "\n",
    "FB + DominantFrequency(low_cutoff=1.0, high_cutoff=3.5)\n",
    "FB + MeanCrossRate()\n",
    "FB + Range()\n",
    "FB + RMS()\n",
    "FB + SampleEntropy(m=2, r=0.5)\n",
    "FB + Autocorrelation(lag=15, normalize=True)\n",
    "FB + IQR()\n",
    "FB + RangeCountPercentage(range_min=0.4, range_max=1.5)\n",
    "FB + ComplexityInvariantDistance(normalize=True)\n",
    "FB + PermutationEntropy(order=3, delay=1, normalize=True)\n",
    "FB + SpectralEntropy(low_cutoff=0.0, high_cutoff=5.0)\n",
    "FB + SpectralFlatness(low_cutoff=0.0, high_cutoff=6.0)\n",
    "FB + Mean()\n",
    "FB + JerkMetric()\n",
    "FB + DimensionlessJerk(log=True, signal_type='acceleration')\n",
    "FB + SignalEntropy()\n",
    "FB + SPARC(padlevel=4, fc=10.0, amplitude_threshold=0.05)\n",
    "FB + LinearSlope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feat, fnames = FB.compute(X, fs=50.0, windowed=True, columns=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame(data=X_feat, columns=fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'num_leaves': [25, 27, 29, 31, 33, 35, 40, 50],\n",
    "    'max_depth': [8, 12, 16],\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.075, 0.1, 0.15, 0.18, 0.2, 0.25],\n",
    "    'min_split_gain': [0., 0., 0.05, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1e-4, 1e-3, 1e-2],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'reg_alpha': [0., 5e-2, 5e-1, 5],\n",
    "    'reg_lambda': [0., 5e-2, 5e-1, 5],\n",
    "}\n",
    "\n",
    "rcv = RandomizedSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=600,\n",
    "    scoring=make_scorer(f1_score),\n",
    "    n_jobs=-1,\n",
    "    cv=zip(training_masks, validation_masks),\n",
    "    refit=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rcv_results = rcv.fit(X_feat, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(data=rcv.cv_results_)\n",
    "res.to_csv('final_lgb_cv_search.csv')\n",
    "\n",
    "pcol = [i for i in res.columns if 'param_' in i] + ['mean_test_score', 'rank_test_score']\n",
    "\n",
    "res.sort_values('rank_test_score').loc[:, pcol].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = res.loc[np.argmin(res.rank_test_score.values), 'params']\n",
    "\n",
    "\n",
    "# with open('lgb_params.txt', 'w') as f:\n",
    "#     for k in best_params:\n",
    "#         f.write(f'{k}: {best_params[k]}\\n')\n",
    "        \n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest = [20, 30, 50, 60, 70, 75, 80, 90, 100, 125]\n",
    "\n",
    "mf1 = []\n",
    "sf1 = []\n",
    "\n",
    "for ne in nest:\n",
    "    clf = lgb.LGBMClassifier(random_state=42, n_estimators=ne, **best_params)\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        clf,\n",
    "        X_feat,\n",
    "        Y,\n",
    "        scoring=make_scorer(f1_score),\n",
    "        cv=zip(training_masks, validation_masks),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mf1.append(np.mean(scores['test_score']))\n",
    "    sf1.append(np.std(scores['test_score'], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.errorbar(nest, mf1, yerr=sf1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "with open('lgb_params.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.strip('\\n').split(': ')\n",
    "        \n",
    "        best_params[parts[0]] = float(parts[1]) if '.' in parts[1] else int(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(random_state=42, n_estimators=125, **best_params)\n",
    "\n",
    "clf.fit(X_feat[training_masks[0]], Y[training_masks[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_feat[training_masks[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], feats.loc[np.argwhere(training_masks[0])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "shap.summary_plot(shap_values[0], feats.loc[training_masks[0]], plot_type='dot')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?shap.summary_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "with open('lgb_params.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.strip('\\n').split(': ')\n",
    "        \n",
    "        best_params[parts[0]] = float(parts[1]) if '.' in parts[1] else int(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(random_state=42, n_estimators=125, **best_params)\n",
    "\n",
    "scorers = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Accuracy': make_scorer(accuracy_score),\n",
    "    'Precision': make_scorer(precision_score),\n",
    "    'Recall': make_scorer(recall_score)\n",
    "}\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf,\n",
    "    X_feat,\n",
    "    Y,\n",
    "    scoring=scorers,\n",
    "    cv=zip(training_masks, validation_masks),\n",
    "    n_jobs=-1,\n",
    "    return_estimator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.923359747665626\n",
      "Recall:     0.9122837910107866\n",
      "Precision:  0.9014838020614788\n",
      "F1:         0.901129880970889\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:  ', np.mean(scores['test_Accuracy']))\n",
    "print('Recall:    ', np.mean(scores['test_Recall']))\n",
    "print('Precision: ', np.mean(scores['test_Precision']))\n",
    "print('F1:        ', np.mean(scores['test_F1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02858763, 0.01016781, 0.0382296 , ..., 0.96583774, 0.95496513,\n",
       "       0.85643185])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-7b13ec7b00ed>:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  f, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 12))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13ec454236b42cfaf4a12078ef46e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46508144746924757 0.44808947441453595 0.21743523426551897\n",
      "0.5109362552505842 0.4285830726465222 0.2693552946963254\n"
     ]
    }
   ],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 12))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = ax[1, 0].plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    ax[1, 0].annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "opt_thresh = []\n",
    "opt_thresh2 = []\n",
    "for i, est in enumerate(scores['estimator']):\n",
    "    y_pred = est.predict_proba(X_feat[validation_masks[i]])[:, 1]\n",
    "    y_true = Y[validation_masks[i]]\n",
    "    \n",
    "    fpr, tpr, trsh = roc_curve(y_true, y_pred)\n",
    "    auc_ = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    imn = np.argmin(np.sqrt(fpr**2 + (1 - tpr)**2))\n",
    "    opt_thresh.append(trsh[imn])\n",
    "    \n",
    "    ax[0, 0].plot(fpr, tpr, label=f'Fold {i+1}: {auc_:.2f}')\n",
    "    \n",
    "    for th, mk in zip([0.4, 0.6, 0.7], ['o', '+', '^']):\n",
    "        idx = np.argmin(np.abs(trsh - th))\n",
    "        ax[0, 0].plot(fpr[idx], tpr[idx], marker=mk, color=f'C{i}')\n",
    "    ax[0, 1].plot(trsh, np.sqrt(fpr**2 + (1 - tpr)**2), label=f'Fold {i+1}: {trsh[imn]:.2f}')\n",
    "\n",
    "    \n",
    "    prec, rec, pr_trsh = precision_recall_curve(y_true, y_pred)\n",
    "    avg_p = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    imn = np.argmin(np.sqrt((1 - prec[:-1])**2 + (1 - rec[1:])**2))\n",
    "    opt_thresh2.append(pr_trsh[imn])\n",
    "    \n",
    "    ax[1, 0].plot(rec, prec, label=f'Fold {i+1}: {avg_p:.2f}')\n",
    "    ax[1, 1].plot(pr_trsh, np.sqrt((1 - prec[:-1])**2 + (1 - rec[1:])**2))\n",
    "\n",
    "print(np.mean(opt_thresh), np.median(opt_thresh), np.std(opt_thresh, ddof=1))\n",
    "print(np.mean(opt_thresh2), np.median(opt_thresh2), np.std(opt_thresh2, ddof=1))\n",
    "    \n",
    "ax[0, 0].plot([0, 1], [0, 1], 'k--')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 0].set_xlim([0, 1])\n",
    "ax[0, 0].set_ylim([0, 1.05])\n",
    "ax[0, 0].set_xlabel('False Positive Rate')\n",
    "ax[0, 0].set_ylabel('True Positive Rate')\n",
    "\n",
    "ax[0, 1].set_xlim([0, 1])\n",
    "ax[0, 1].set_ylim([0, 1.05])\n",
    "ax[0, 1].set_xlabel('Threshold')\n",
    "\n",
    "ax[1, 0].legend()\n",
    "ax[1, 0].set_xlim([0, 1])\n",
    "ax[1, 0].set_ylim([0, 1.05])\n",
    "ax[1, 0].set_xlabel('Recall')\n",
    "ax[1, 0].set_ylabel('Precision')\n",
    "\n",
    "ax[1, 1].set_xlim([0, 1])\n",
    "ax[1, 1].set_ylim([0, 1.05])\n",
    "ax[1, 1].set_xlabel('Threshold')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
